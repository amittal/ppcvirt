\documentclass[10pt,twocolumn]{article}

\usepackage{times}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{epstopdf}

\begin{document}

\title{Efficient Virtualization on Embedded Power Architecture\textsuperscript{\textregistered} Platforms}
\author{}
\date{}
\maketitle
\thispagestyle{empty}

\maketitle
\begin{abstract}
  Power Architecture\textsuperscript{\textregistered} is popular and widespread on embedded systems, and such
  platforms are
  increasingly
  being used to run virtual machines\cite{embedded_virtualization}. While the Power
  Architecture meets the
  Popek-and-Goldberg virtualization requirements for traditional trap-and-emulate
  style virtualization, the performance overhead of virtualization remains high.
  For example, workloads exhibiting a large amount of kernel activity typically
  show 3-5x slowdowns over bare-metal.

  Recent additions to the Linux kernel contain guest and host side paravirtual
  extensions for Power Architecture. While these extensions improve performance
  significantly, they
  are guest-intrusive, non-portable and cover only a subset of all possible
  virtualization optimizations.

  We present a set of host-side optimizations that achieve comparable
  performance
  to the aforementioned paravirtual extensions, yet being
  guest-neutral. Our optimizations are based on adaptive binary translation.
  Unlike the paravirtual approach which requires extensive
  guest modifications, the only
  guest modification required by us is to include a guest-side virtual device driver
  to create a shared address space between the guest and the host.
  We implement our ideas in a prototype based on Qemu/KVM.
  After our modifications, KVM can boot a Linux guest around 2.5x faster. Our solution
  provides equivalent performance to
  the paravirtual approach, without being guest-specific.
\end{abstract}
\section{Introduction}
Embedded devices based on Power Architecture are dominant for their
favourable power/performance characteristics. Virtualization on these platforms is
compelling for several applications\cite{embedded_virtualization}. While newer Power
Architecture platforms
have explicit support for efficient virtualization\cite{freescale_embedded_hyperv, hwassists_hyperv}, a majority of
prevalent embedded devices run on older (and cheaper) Power Architecture platforms that use
traditional trap-and-emulate style virtualization\cite{popekgoldberg}. Hence, efficient
virtualization is highly desirable on these platforms.

The current virtualization approach on Power Architecture platforms uses traditional
trap-and-emulate. The guest operating system is run unprivileged, causing each
execution of a privileged
operation to exit into the hypervisor. For guest workloads executing a large number
of privileged instructions, these VM exits are a major performance
bottleneck. Table~\ref{tab:kvm_performance} lists the performance of vanilla Linux/KVM
on a few common workloads, comparing them with bare-metal performance.
For example, a guest Linux boot takes almost 5x longer when run virtualized.

The poor performance of simple trap-and-emulate style virtualization has led to
the inclusion of paravirtual extensions in the Linux kernel on both
guest and host sides for Power Architecture\cite{pvpower}. The paravirtual extension in the guest
rewrites the guest (binary) kernel
code at startup time to replace most privileged instructions with
hypervisor-aware unprivileged counterparts ({\em hypercalls}).
At guest startup, the guest creates a shared address space with
the host through a hypercall. This shared address space
is used in future to efficiently exchange arguments and results of other hypercalls
between guest and host.
Table~\ref{tab:kvm_performance} lists KVM performance after enabling paravirtual
extensions in the guest and the host. The performance improves significantly over
unmodified KVM.

The paravirtual approach has shortcomings. Firstly, extensive guest
modifications are required, which makes the optimizations highly Linux specific.
Secondly, all guest
privileged instructions are rewritten only at kernel startup time. Hence,
optimizations are
not possible for privileged code running as loadable kernel modules. The
current paravirtual
approach on Linux also fails in the presence of self-referential, self-modifying
and dynamically generated code. The Linux paravirtual extensions carefully
avoid such pitfalls by ensuring that the kernel never uses self-referential
or self-modifying code after it has been rewritten. However, such constraints are
ungraceful, and hard to maintain over time.

We propose a host-side adaptive binary translation mechanism to optimize guest
privileged instructions at runtime. Our approach is more general and
makes no assumptions about guest OS. We require minimal guest
modifications (in the form of a device driver to setup shared address spaces)
and achieve
comparable performance to the paravirtual approach. Because we translate
guest code at runtime (and not at kernel startup time, as done in the paravirtual
approach), we
can optimize code in both the main kernel and the loadable modules. Our approach also
does not assume anything about the guest, and ports seamlessly to different
guests or different versions of the same guest.
The last column in Table~\ref{tab:kvm_performance} summarizes the performance results of
our host-side binary translation approach.

Our host-side virtualization optimizations are based on adaptive binary
translation. On observing a large number of VM exits by a guest instruction, we translate
that instruction {\em in-place} to directly execute the corresponding
VMM logic (thus avoiding an exit). In doing so, we directly modify the guest's
address space. This is in contrast to a {\em full} binary translation approach
that translates the entire guest code
(e.g., VMware's x86-based binary translator\cite{adams:asplos06}). Our approach is
simpler and incurs less overhead than full binary
translation approaches. We compare the two approaches in more detail in
Section~\ref{sec:comparison_with_full_bt}.

Modifying the guest's address space has obvious pitfalls.
Firstly, we must ensure correctness in presence
of arbitrary branches in the code. For example, it would be incorrect if the guest
could potentially jump to the middle of our translated code.
%We rely on the fixed-length
%word-aligned nature of Power Architecture instructions to achieve correctness.
To ensure correctness, we replace a privileged guest instruction
by at most one translated instruction in the guest's address space. Because instructions
are fixed length and word aligned on Power Architecture, this ensures that
there can never be a branch to the middle of our translated
code. Any branch could only reach either the beginning or the end of our
replacement instruction.

Not all guest privileged instructions can be emulated by just one
replacement instruction.
Such instructions are instead replaced with a branch to a code
fragment in a host-managed translation cache. This branch is implemented as a
single instruction in the guest's address space, and the translation
cache is allocated in host's address space. We provide a mechanism for the guest
to directly access the translation cache in host's address
space (details in Section~\ref{sec:bintrans}).

The second pitfall with in-place guest modification is due to
self-referential and self-modifying code inside the guest. We maintain correctness by 
marking the pages containing the modified instructions {\em execute-only}. This
causes the hardware to trap into the VMM on any guest read/write access to the
modified code. We call this mechanism {\em read/write tracing}.

Finally, read/write tracing can cause a large number of
page faults, especially due to false sharing. The problem is
exacerbated on embedded Power Architecture platform, where OS typically uses
huge pages to reduce TLB pressure. We found that such page faults can
significantly reduce performance.
We implement two important optimizations to address this problem, namely
{\em adaptive page resizing} and {\em adaptive data mirroring}.

In summary, this paper presents an efficient host-side optimization solution for
Power Architecture virtualization. Our approach is based on in-place binary translation,
and maintains guest correctness in presence of self-referential and self-modifying
guest code.
The paper is organized as
follows. Section~\ref{sec:performance_char} characterizes the performance of
KVM on Power Architecture and discusses the typical sources of overhead.
Section~\ref{sec:bintrans} discusses our in-place binary translation approach.
We discuss read/write tracing and our optimizations to make
it efficient in Section~\ref{sec:tracing}.
Section~\ref{sec:results} presents our experiments and results, and
finally Sections~\ref{sec:discussion}-\ref{sec:conclusion} conclude.

\section{Performance Characterization of KVM on Power Architecture Platforms}
\label{sec:performance_char}
We perform our experiments on Linux/KVM running on an
embedded Power Architecture processor from Freescale used in network
routing.
The virtualization overheads
of trap-and-emulate style virtualization can be up to 15x for compute-intensive
workloads executing a large number of privileged
instructions (Table~\ref{tab:kvm_performance}). The primary source of
overhead are VM-exits due to guest privileged instructions.
Table~\ref{tab:priv_opcodes} lists the most executed privileged
opcodes and briefly explains their semantics.
Figure~\ref{fig:opcode_exit_fraction} shows the percentage of exits caused
due to each opcode. Only five opcodes result in more than 80\% of exits on all
three benchmarks. Table~\ref{tab:linuxboot_exitcount} presents the frequency
profile of VM exits on the Linux boot benchmark in more detail.

We next profile the number of distinct program counter (PC) values that
cause exits. Figure~\ref{fig:pc_profile} shows a histogram on the number
of distinct PC values and the frequency of exits on them.
Table~\ref{tab:linuxboot_pcexits} presents the exit profile of different
PCs in more detail for the Linux boot benchmark. For example, around 92\% of all
exits are caused by only 93 distinct PCs for guest Linux boot. These
measurements confirm that
binary translating
only the most frequently executed opcodes/PCs is likely to produce large
improvements.

\begin{table}[!b]
\centering
\caption{Sources of VM Exits: Opcodes and Exceptions}
     \begin{tabular}{|l | p{5cm} |} \hline
       Opcode \verb, , & Description \\ \hline
       {\tt mfmsr} & Move from machine state register \\ \hline
       {\tt mtmsr} & Move to machine state register \\\hline
       {\tt mfspr} & Move from special purpose register \\\hline
       {\tt mtspr} & Move to special purpose register \\\hline
       {\tt wrtee(i)} & Write MSR External Enable  \\\hline
       {\tt rfi} & Return from Interrupt \\\hline
       {\tt tlbwe} & Writes a TLB entry in hardware\\\hline
       \hline
       Exception \verb, , & Description \\ \hline
       {\tt dtlbmiss} & Page fault on data due to TLB not present\\    \hline
       {\tt itlbmiss} & Page fault on instruction due to TLB not present\\    \hline
       {\tt dsi} & Page fault due to insufficient privilege\\\hline

     \end{tabular}
\label{tab:priv_opcodes}
\end{table}

\begin{figure}[!htb]
\centering

\includegraphics[scale=0.5]{exit_count.eps}
\caption{Sources of VM exits}
\label{fig:opcode_exit_fraction}
\end{figure}

\begin{figure}[!htb]
\centering

\includegraphics[scale=0.5]{multiple_ins_patching.eps}
\caption{Multiple instruction patching}
\label{fig:multiple_insns_patching}
\end{figure}

\begin{table}[!b]
\centering
\caption{Sources of VM exits for Linux boot}
     \begin{tabular}{lcc} \hline
       Instruction class  & Exit count & \% of total exits  \\ \hline
       {\tt mfspr} & 4484245 & 33.8  \\
       {\tt wrtee} & 2792109 & 21.1  \\
       {\tt mtspr} & 2307647 & 17.4  \\
       {\tt rfi} & 575302 & 9.5 \\
       {\tt mtmsr} & 413847 & 4.3 \\
       {\tt tlbwe} & 391813 & 3.1 \\
       %DTLBREAL & 247282 & 2.9 \\
       %ITLBREAL & 241070 & 1.8 \\
       {\tt dtlbmiss} & 198239 & 1.5 \\
       {\tt itlbmiss} & 192046 & 1.4 \\
       \hline
     \end{tabular}
\label{tab:linuxboot_exitcount}
\end{table}

\begin{figure}[!htb]
\centering

\includegraphics[scale=0.5]{pc_count.eps}
\caption{Exit Profile of Different Guest PCs}
\label{fig:pc_profile}
\end{figure}

\begin{table}[!b]
\centering
\caption{PCs responsible for VM exits for linux boot}
     \begin{tabular}{lcc} \hline
       Exits count  & PC count & \% of total exits  \\ \hline
       $>$20000 & 93 & 91.9  \\
       $>$10000 & 23 & 3.1  \\
       $>$5000 & 68 & 4.2  \\
       $>$2000 & 12 & 0.3 \\
       $>$1000 & 17 & 0.2 \\
       $<$1000 & 299 & 0.2 \\
       \hline
     \end{tabular}
\label{tab:linuxboot_pcexits}
\end{table}


\begin{table*}
\centering
\caption{Performance comparison. The second column (KVM) represents
unmodified KVM. The third column (KVM-PV) represents KVM with paravirtual
extensions. The last column (KVM-BT) represents KVM with adaptive
binary translation with all read/write tracing optimizations (our approach)}
      \begin{tabular}{|l| l|p{5cm} | r r r r|} \hline
	        S.No.\verb, ,&  Benchmark\verb, ,& Description  & Bare-metal \verb, ,& KVM \verb, , & KVM-PV \verb, ,& KVM-BT \\ \hline

     &&& \multicolumn{4}{c|}{ Running Time in $sec$}\\\cline {4-7}  
      1&  Linux Boot& linux boot & 6.5	&	30.03	&	11.79	&	12.24 \\ \hline
      2& Echo Spawn	& Spawning a large number of echo processes&1.4	&	21.34	&	6.5	&	7.26\\\hline
      3& Find	& find / -name temp & 0.39	&	1.89	&	0.67	&	0.71 \\ \hline
	   \multicolumn{3}{|c}{ lmbench microbenchmarks }& \multicolumn{4}{c|}{Latency in $msec$}\\  \hline

4	&	syscall	&	 measures time to write one word to /dev/null	&	0.000	&	0.020	&	0.003	&	0.003	\\	\hline
5	&	stat	&	 measures the time to invoke the stat system call	&	0.003	&	0.033	&	0.006	&	0.007	\\	\hline
6	&	fstat	&	time to invoke fstat system call on open file 	&	0.001	&	0.021	&	0.004	&	0.004	\\	\hline
7	&	open/close:	&	 time to open a temporary file for reading and closing it immediately 	&	0.006	&	0.067	&	0.013	&	0.014	\\	\hline
8	&	sig hndl	&	 time to install a signal handler	&	0.001	&	0.024	&	0.004	&	0.004	\\	\hline
9	&	pipe 	&	 round trip latency to pass a word from process A to process B and back to A	&	0.014	&	0.164	&	0.032	&	0.034	\\	\hline
10	&	fork	&	 fork and exit 	&	1.084	&	6.641	&	1.640	&	2.061	\\	\hline
11	&	exec	&	 fork, exec and exit	&	3.065	&	20.543	&	6.254	&	7.533	\\	\hline
12	&	sh	&	 fork, exec sh -c and exit	&	6.645	&	45.164	&	13.842	&	16.327	\\	\hline



        \hline
      \end{tabular}
\label{tab:kvm_performance}
\end{table*} 
%\begin{table}[!b]
%\centering
%\caption{Performance of Base KVM}
%     \begin{tabular}{lcc} \hline
%       Benchmark  & Performance (\% of bare-metal) \\ \hline
%       Linux Boot & 21.7 \\
%       Echo Spawn & 6.4 \\
%       Find & 20.6 \\
%       \hline
%     \end{tabular}
%\label{tab:kvmperformance}
%\end{table}

\begin{table}[!b]
\centering
\caption{Tracing overhead optimizations - Approaches}
     \begin{tabular}{|l | p{5cm} |} \hline
       Approach \verb, , & Description \\ \hline
       BT-Trace & In-place Binary Translation and Read/Write Tracing \\ \hline
       Avoid-RPF & Avoiding Read Page Faults by copying data and translating faulting instructions  \\\hline
	   Adapt-TLB & Adaptive TLB Resizing  \\\hline

     \end{tabular}
\label{tab:diff_approaches}
\end{table}

\begin{table*}
\centering
\caption{Performance comparison: Refer Table \ref{tab:diff_approaches}}
      \begin{tabular}{|l| r r r r |} \hline
	         Benchmark\verb, ,& KVM \verb, , & BT-Trace \verb, ,& Adapt-DM \verb, , & Adapt-PR  \\ \hline

     & \multicolumn{4}{c|}{ Running Time in $sec$}\\ \cline {2-5}  
Linux boot	&	30.03	&	13.10	&	12.24	&	13.29	\\
Echo Spawn	&	21.34	&	8.25	&	7.26	&	7.93	\\
Find	&	1.89	&	0.92	&	0.71	&	1.15	\\
\cline{2-5}
	   
     & \multicolumn{4}{c|}{Latency in $msec$}\\  \cline{2-5}
syscall	&	0.020	&	0.008	&	0.003	&	0.010	\\
stat	&	0.033	&	0.012	&	0.007	&	0.006	\\
fstat	&	0.021	&	0.009	&	0.004	&	0.004	\\
open/close:	&	0.067	&	0.024	&	0.014	&	0.031	\\
sig hndl	&	0.024	&	0.009	&	0.004	&	0.004	\\
pipe 	&	0.164	&	0.054	&	0.034	&	0.135	\\
fork	&	6.641	&	2.240	&	2.061	&	2.823	\\
exec	&	20.543	&	8.384	&	7.533	&	8.849	\\
sh	&	45.164	&	17.615	&	16.327	&	21.813	\\

 \hline
      \end{tabular}
\label{tab:detailed_results}
\end{table*} 

\begin{table*}
\centering
\caption{Sources of VM Exits}
      \begin{tabular}{|@{}p{1cm}|p{1cm}|p{1cm}|@{}c@{}| p{0.8cm}| p{1cm}|@{}c@{}| p{1cm} | p{1cm}|@{}c@{}| p{1cm} |p{1cm}|@{}c@{} |} \hline
	         &  \multicolumn{3}{c|} {KVM}& \multicolumn{3}{c|} {BT-Trace} & \multicolumn{3}{c|} {Adapt-DM} \verb, ,& \multicolumn{3}{c|} {Adapt-PR} \\ \hline

     &Priv. Exits&TLB VIRT&DSI &Priv. Exits&TLB VIRT&DSI &Priv. Exits&TLB VIRT&DSI &Priv. Exits&TLB VIRT&DSI \\ \hline  
Boot	&	12085062	&	458339	&	13721	&	44163	&	335125	&	455761	&	43123	&	327477	&	18036	&	47500	&	348289	&	456107	\\	\hline
Echo Spawn	&	9850769	&	417554	&	16999	&	36950	&	525249	&	513745	&	35849	&	503232	&	17999	&	36657	&	488791	&	516483	\\	\hline
Find	&	788358	&	2106	&	41	&	909	&	2318	&	96931	&	737	&	2290	&	39	&	1114	&	192217	&	18291	\\	\hline
hanoi	&	26125	&	507	&	25	&	820	&	670	&	1115	&	818	&	619	&	23	&	825	&	1391	&	695	\\	\hline
context	&	460756	&	674	&	33	&	33334	&	1200	&	195372	&	52565	&	819	&	31	&	20328	&	248285	&	16872	\\	\hline
syscall	&	54274	&	932	&	45	&	171	&	1117	&	23640	&	157	&	1090	&	44	&	166	&	1090	&	21597	\\	\hline
stat	&	49683	&	946	&	45	&	158	&	1100	&	15449	&	155	&	1073	&	43	&	160	&	3019	&	42	\\	\hline
fstat	&	51839	&	945	&	43	&	165	&	1117	&	21201	&	158	&	1109	&	44	&	162	&	2964	&	44	\\	\hline
open/ close	&	55420	&	933	&	44	&	169	&	1102	&	15342	&	169	&	1080	&	43	&	185	&	23712	&	43	\\	\hline
sig hndl	&	52128	&	945	&	39	&	162	&	1140	&	19541	&	156	&	1078	&	260	&	165	&	1074	&	19478	\\	\hline
pipe	&	918788	&	1146	&	52	&	39170	&	1335	&	229665	&	60450	&	1322	&	51	&	21718	&	261374	&	17807	\\	\hline
fork	&	68775	&	2575	&	173	&	543	&	5825	&	6579	&	530	&	5712	&	345	&	548	&	14987	&	347	\\	\hline
exec	&	149439	&	6430	&	239	&	638	&	7969	&	7209	&	624	&	8075	&	238	&	640	&	18119	&	241	\\	\hline
sh	&	295751	&	12952	&	441	&	1179	&	15699	&	14706	&	1151	&	15746	&	439	&	1161	&	34827	&	442	\\	\hline
      \end{tabular}
\label{tab:vm_exit_stats}
\end{table*} 



\section{In-place Binary Translation}
\label{sec:bintrans}
We translate guest instructions in-place. These translations could potentially
require code fragments involving multiple instructions. To ensure correctness, we
must place these code fragments in {\em reserved} space (i.e., not accessed by
guest for other purposes). We require the guest to explicitly reserve this space
and inform the host through a hypercall. We first discuss the setup of this shared
address space between the guest and host.
\subsection{Shared Guest-Host Address Space}
\label{sec:sharedspace}
The guest reserves some pages in it's address space to store the
binary translator's translation
cache. The guest also reserves some pages to store it's emulated registers.
Both the translation cache and guest's emulated registers are maintained by
the host. Hence the host must also have access to these pages, and hence these
are {\em shared address spaces}.

We require the guest to reserve these pages at startup and inform the host about
them using a hypercall.
This is the only guest modification we require.
We implement this page reservation
and hypercall as a virtual guest device driver (similar to the ``tools'' mechanism
used in many virtualization solutions).
We require XXX LOC changes in a Linux guest to implement this mechanism. This is
significantly less than the XXXX LOC required by the Linux paravirtual
extensions.

In our current implementation, two distinct address ranges are reserved by the
guest for our BT optimizations.
The first one is a 4KB page with guest read/write privileges
to store guest's emulated registers.  The second address range is 32KB long
with guest execute privileges to store the translation cache.

These shared address spaces also have placement constraints. For easy direct
addressing, we place the 4KB page (containing guest's emulated registers) at
the top of guest's virtual address space (at {\tt 0xfffff000}). This is important 
because direct memory-addressing modes on Power Architecture specify only 16-bit
addresses. For absolute addressing, this implies that the memory
address must lie in either
the lower 32KB ({\tt 0x00000000-0x00007fff}) or higher
32KB ({\tt 0xffff8000-0xffffffff}) of the virtual address space. Because the lower
addresses are reserved for user applications on Linux, we use the higher addresses.

The execute-only space (32KB) also has placement
constraints. The execute-only space must be reachable
by a direct branch instruction from the guest code. A direct branch instruction
on Power Architecture specifies a 26-bit
word-aligned address. The branch target addressing is either PC-relative
or absolute. This implies that the translation cache could be
in one of the following regions: {\tt 0x0000000-0x00ffffff}, {\tt 0xff000000-0xffffffff},
or within a $\pm$32MB distance of any instruction in guest's kernel code sections.
Also, as we explain in the
next section (Section~\ref{sec:bintrans}), the code in the translation cache must
be able to branch
back to the guest's code (to the instruction following the translated instruction).
Because guest's code could be arbitrarily placed, this constrains the translation
cache to be within $\pm$32MB distance of any instruction in
guest's kernel code section.

The guest reserves a 32KB execute-only space for the
translation cache within 32MB of the kernel's code
section. Given that kernel code sections are small (typically 5-6MB),
this constraint is usually easy to satisfy. If the execute-only
pages happen to lie beyond 32MB distance of a privileged instruction, that
instruction is precluded from being optimized. Other instructions that lie
within 32MB of the translation cache can still be optimized.

Finally, we note that requiring a guest-side device driver for performance
is a well-accepted virtualization practice. For example, x86-based hypervisors
typically require installation of ``tools'' in the guest for better performance.

\subsection{Translation of Privileged Instructions}
Some privileged instructions can
be emulated by single-instruction translations. For example, {\tt mfmsr} is translated
to a {\tt load} instruction to the address of the emulated {\tt msr}
register in the shared read/write page. The opcodes which can be
translated to single instructions are XXX.

Other privileged opcodes require translation to multiple instructions.
For such opcodes, we store the emulation code in the translation cache, and
patch the original instruction with a {\tt branch} instruction to jump to it's
emulation code. We call the privileged instruction
that was patched, the {\em patch-site}. The emulation code in the
translation cache is terminated
with another branch {\em back} to the instruction following the patch-site.
(see Figure~\ref{fig:txcache}). Because each patch-site requires a different
terminating branch instruction, a new translation is generated for
each patch-site.

%We use Power Architecture's unconditional direct {\tt branch} opcode to jump to the
%translation cache from the patch-site. The {\tt branch} opcode specifies a 24-bit
%branch offset, with either absolute or relative addressing. This constrains us
%to place the emulation code either within a 24-bit distance of the patch-site (i.e.,
%$\pm$16MB) or in the top or bottom 16MB of the guest's virtual address
%space (see Figure~\ref{fig:branchtargets}). We ensure this by mapping the shared
%page containing our translation cache within $\pm$16MB of the kernel's code
%section (as also mentioned in Section~\ref{sec:sharedspace}). XXX Discuss loadable
%modules and how they are handled.

This branch back to the instruction following the patch-site is
the primary reason behind constraining the translation cache to lie within
$\pm$32MB of the patch-site.
We investigated another possibility to avoid this constraint on the placement
of translation cache. Instead of using a direct {\tt branch} opcode, we used
the {\tt bl} opcode to jump to the translation cache. The {\tt bl} opcode is
identical to the {\tt branch} opcode, except that it additionally stores the
address of the following instruction in the link register {\tt lr}. This instruction
is meant to implement function calls (return address stored in {\tt lr}).
The {\tt bl} instruction
allows greater flexibility in the placement of translation cache. For example, it is
now possible to place the translation cache in the top 32MB of the address
space ({\tt 0xfe000000-0xffffffff}) and use absolute addressing for the {\tt bl}
instruction. For the branch back, we can simply use the {\tt blr}
instruction (branch to the address in {\tt lr}). {\tt blr} allows specification
of a full 32-bit address in {\tt lr} and can potentially jump anywhere in the
address space.
This solution however
clobbers the {\tt lr} register, which can deviate guest's behaviour.
We next tried saving/restoring the
{\tt lr} register at the patch-site before/after branching to the emulation code.
To do this, we patched not just the privileged instruction but also 1-2
surrounding instructions (see Figure~\ref{fig:multiple_insns_patching}). The
surrounding instructions are made part of the emulation code to preserve
guest behaviour. This can
be done safely only if the surrounding instructions are not control-flow instructions.

This {\tt bl}-based solution works correctly for emulation of the privileged opcode.
However, if the guest ever jumps to one of the surrounding instructions (which we
replaced), the guest will behave incorrectly.
Because it is practically impossible to predict all possible branch targets at
translation time (due to presence of indirect branches), we discarded this potential
solution.

The hypervisor maintains the state of the translation cache in
a hash table indexed by the PC value of current patch-sites. Each hash table
entry stores patch-site PC, the patch-site's original contents,
the location of the corresponding emulation code in the shared page, and other
information useful
for cache replacement. In our implementation, we use the FIFO cache replacement policy
for it's simplicity and high performance.
\section{Read/Write Tracing}
\label{sec:tracing}
Changing instructions in the guest address space can cause
inconsistencies if the guest tries to read or write it's own code.
For this reason, we protect the pages containing patch-sites with hardware
page-protection bits. Embedded Power Architecture platforms provide three {\tt rwx}
protection bits per page for both user/supervisor privilege levels. Using these
bits, we can mark a guest page containing a patch-site
execute-only in user mode. This allows the
execution of an instruction on this page to proceed uninterrupted,
but any read or write access causes a page fault (and a VM exit).
On a page fault, the hypervisor
emulates the faulting instruction in software. We call this
method memory read/write tracing (similar to VMware's memory tracing
on x86\cite{adams:asplos06}).

We implement software emulations of memory instructions in KVM. There are XXX
different memory opcodes on Power Architecture that need to be emulated.
For read instructions, we simply return the original contents of the memory
address in the appropriate destination
operand. The original contents may be obtained either from the present guest
page (if the address does not intersect with a patch-site) or from the hypervisor's
hash table (if the address matches a patch-site) or both.
For write instructions, if the memory address (and length) intersects with a patch-site,
we invalidate and free the corresponding translation cache
entry and replace the guest page with it's original contents before
restarting the instruction. If the memory address does not intersect with a
patch-site, we simply perform the equivalent write operation to the guest's memory.

The overhead of read/write tracing depends on the number of accesses by the guest
to it's pages containing kernel code. For a Linux guest, we found that this overhead
can be quite significant.
Much of this overhead is due to false sharing. For example, a large number of page
faults occur because guest kernel data often resides on the same page as
guest kernel code. The problem becomes worse with increasing page size. Linux
on embedded
Power Architecture uses huge pages for kernel code/data to minimize
TLB pressure, causing our tracing mechanism to result in a huge number of page
faults.

We implement two optimizations to reduce tracing page faults. Our first optimization
adaptively resizes the guest's pages to reduce false sharing.
Our second optimization adaptively
mirrors guest data (which is causing a large number of faults) to reduce the number
of page faults caused by read accesses (which is the common case). For the second
optimization we also translate the faulting instruction to access the mirrored data.
We discuss both optimizations in detail below.

\subsection{Adaptive Page Resizing}
Typical TLB sizes on embedded Power Architecture processors are small.
For example, the software-managed TLB on our system is a combination of a 16-entry
fully-associative cache of variable-sized page table entries
and a 512-entry 4-way set-associative fixed-size (4KB) page table entries.
A faster L1 TLB lookup cache is implemented in hardware, and all invalidations
to maintain consistency with the software-programmed L2 TLB are done automatically.
The variable-pagesize TLB cache supports 11 different page sizes: 4K, 16K, 64K,
256K, 1M, 4M, 16M, 64M, 256M, 1G, and 4G. Further a page of size S must be S-byte
aligned in physical and virtual address spaces.

The hypervisor traps guest's accesses to the TLB, and creates appropriate
{\em shadow TLB entries}. These shadow entries are loaded into hardware
while the guest is
running. This mechanism is similar to the hardware-managed shadow page
tables used in x86\cite{adams:asplos06}. The guest cannot directly access the shadow
TLB entries, as guest's accesses to the TLB entries are trapped and emulated by
the hypervisor. This allows the hypervisor full flexibility in choosing the
size and privileges of it's shadow TLB entries. For example, the hypervisor
can setup multiple shadow TLB entries, to shadow a single guest TLB entry representing
a larger page.
To minimize TLB pressure, the hypervisor typically uses
one shadow TLB entry per guest TLB entry. For example, if the guest uses 4MB pages,
then the shadow TLB will also have corresponding entries for 4MB pages. We
implement read/write tracing by disabling read/write privileges in the shadow TLB entry.

The guest OS typically uses huge pages for the kernel space to
reduce TLB pressure. For example, our Linux guest
uses just one TLB entry of size 256MB to map all its code and data sections.
Disabling read/write privileges from this TLB entry predictably causes an unacceptably
large number of page faults (every kernel data access becomes a page fault).

We resize shadow TLB entries to deal with this overhead of page faults due to read/write
tracing. On patching a privileged guest instruction, we {\em break} the TLB entry
containing this guest instruction into smaller TLB entries
(a {\em smaller TLB entry} means a TLB entry for a smaller page). After breaking,
we only mark the
TLB entry containing the patch-site execute-only, and leave the remaining
unmodified.
To minimize false sharing, we try and minimize the
size of the page containing the patch-site. All other pages created by
breaking the large page, are always larger than this page and are sized
to minimize
the overall number of TLB entries. While smaller pages reduce false sharing, they
also result in increased TLB pressure.

Breaking a large page potentially creates
many smaller pages due to alignment restrictions.
For example, if the kernel has mapped
itself using a 256MB page at virtual address (0xc0000000,0xd0000000), and a
patch is to be applied at address 0xc0801234, and we have decided to break
the patch-site page into a 4MB page, the new set of TLB entries will be for
addresses
(0xc0000000,0xc0800000);(0xc0800000,0xc0c00000);
(0xc0c00000,0xc1000000);(0xc1000000,0xc2000000);
(0xc2000000,0xc4000000);(0xc4000000,0xc8000000);
(0xc8000000,0xd0000000).

The size of the smaller page depends on the trade-off between the number
of tracing page faults (due to false sharing) and the increase in TLB pressure
due to this page fragmentation.
%This three-way trade-off between the number of
%privileged exits, the number of tracing page faults, and TLB thrashing is
%delicate and requires careful performance tuning.
We use the following policy to determine the size of the smaller TLB entries. On the
creation of a new patch-site (due to a VM-exit on a privileged instruction) or a
large number of page faults due to read/write tracing,
the size of the page
containing the corresponding patch-site is halved, subject to
a minimum page size of 256K.
The lower bound on page size (256K) limits the maximum number of TLB entries.

To reduce TLB pressure, TLB entries with identical privileges are
opportunistically merged into
larger entries. For example, neighbouring TLB entries are merged if the
removal of a patch-site due to translation cache replacement causes
them to have identical {\tt rwx} privileges.

Adaptive page resizing significantly reduces the number of page faults. For
example, a Linux guest boots in 13.29 seconds (as opposed to 30.03 seconds on
unmodified KVM) with in-place binary translation
and adaptive page resizing for read/write tracing. Without adaptive page resizing,
the guest livelocks at boot time due to the large number of
page faults. A detailed study of the performance tradeoffs is presented in
Section~\ref{sec:results}.

\subsection{Adaptive Data Mirroring}
After implementing adaptive page resizing,
we still observed 10-20\% performance overhead due
to tracing page faults. We found that a major cause of these page faults were
read accesses to function dispatch tables and exception handler tables which were
colocated with kernel code. Often these tables share
the same 256KB region as the privileged kernel code accessing them, rendering our
adaptive page resizing algorithm ineffective on these accesses.

We avoid these faults by dynamically monitoring such page faults, adaptively
copying the data being accessed to host's shared address space, and
translating the instructions accessing this data to access from the new location.
This optimization prevents future page faults on this data.

We perform this optimization if the number of tracing page faults
due to a certain read-access instruction exceeds a threshold. On noticing
such faults, we copy the data being accessed
to a ``data cache''. This data cache is maintained in the read/write shared page
also storing the guest's emulated registers.
The faulting instruction is replaced with
code to check the data cache for the address being read. If the check succeeds, the
translated code reads the data from the cache, thus avoiding a page fault.
If the check fails, the translated code executes the original
instruction just as before (which may result in a page fault just as before).
If the hit rate of the data cache is high, we have reduced the number of page faults.
This translation code of the faulting instruction is stored in the translation
cache (maintained in the execute-only shared space) and the faulting
instruction is replaced with a branch to this code.

To maintain guest correctness, the pages containing patches for
faulting instructions (due to tracing) need to be read/write traced too.
This can potentially result in a chain-effect: tracing of these new pages
can cause more page faults, resulting in more patching to be performed, and so on\ldots
Fortunately, we do not see this chain effect in practice. The faulting instructions
that are patched typically reside on the same
page as the original patched privileged instructions, causing this
cycle to converge on the first iteration (because these pages are already traced).
Even if the patching of the faulting instructions requires a new read/write trace to be
created, we expect this
cycle to converge in 1-2 iterations. Intuitively, kernel
code which causes privileged
VM-exits or tracing page faults is likely to be spatially close, and will eventually
lead to a small set of traced pages. We observed this behaviour in all our experiments
with Linux guests.

Finally, we note that this optimization is only valid for read accesses. For write
accesses, we must perform the update at the original data location, as other
instructions (which have not been translated to access the new data location) may
read from the original location.
\section{Experimental Results}
\label{sec:results}
In summary, we have implemented the following optimizations in Linux/KVM:
\begin{enumerate}
  \item In-place Binary Translation and Read/Write Tracing
  \item Adaptive Page Resizing
  \item Adaptive Data Mirroring and Translation of Faulting Instructions
\end{enumerate}
We perform experiments to measure the performance improvements through these
optimizations. Optimization 1 results in reduced number of VM exits due
to privileged instructions. For a guest/hypervisor using small pages, this results
in performance improvements. Most guests however use large pages to reduce
TLB pressure. Tracing large pages causes a page fault on each kernel data access.
This leads to unacceptably low performance. For example, a Linux guest boot livelocks
on such a system.

Optimization 2 adaptively resizes the shadow TLB entries to reduce
the number of tracing-induced page faults and results in
performance improvements over unmodified KVM. The XXX column
in Table~\ref{tab:detailed_results} shows the performance of KVM with
Optimizations 1~and~2.
Different workloads show different improvements. The improvement primarily depends
on a three-way tradeoff between the number of VM-exits, the number of
tracing page faults, and the number of page faults due to increased TLB pressure.
In our experience, this delicate three-way tradeoff requires careful
performance tuning.
Table~\ref{tab:vm_exit_stats} shows the number of VM-exits and page faults due
to all these three reasons for each workload, before and after our optimizations.
XXX Discuss the changes in the number
of VM exits and page faults for different workloads.
%For many workloads, the increase in tracing-induced page faults leads
%to significant (around 10\% for Linux guest boot) performance overhead.

Optimization 3 further reduces the number of tracing page faults by
making copies of frequently read data residing in traced pages. The XXX column in
Table~\ref{tab:vm_exit_stats} confirms this reduction on almost all workloads.
XXX Discuss some workloads. Overall,
Optimization 3 results in a XXX\% average performance
improvements on our macrobenchmarks.



We next measure the overhead of our adaptive page resizing algorithm. To
measure overhead, we statically configured the shadow TLB to the best
possible configuration (number of kernel TLB entries, size of each TLB
entry, etc.). We found experimentally that for a Linux guest with 5MB code
section, the best configuration is to have a 5 distinct pages, one of size
4MB ({\tt 0xc0000000-0xc03fffff}), and
four of size 1MB each (covering {\tt 0xc0400000-0xc07fffff}). We compared the
performance of
our statically configured TLB configuration with that of our adaptive resizing
algorithm.
Table~\ref{tab:adaptive_tlb_resizing} summarizes our results for the macrobenchmarks.
We observe that our resizing algorithm performs within 1-2\% of the optimal.
Our algorithm also settles on the same optimal configuration; the overhead is
primarily due to the few extra page faults in the beginning to reach this configuration.

\section{Discussion}
\label{sec:discussion}
\subsection{Comparison with Full Binary Translation}
\label{sec:comparison_with_full_bt}
We call a binary translator which translates all guest instructions a {\em full}
binary translator.
VMware's x86-based binary translator\cite{adams:asplos06} is an example of
such a system. A full binary translator translates all guest code, and not
just the privileged instructions (as done in our system). The advantage of
our approach is it's simplicity and often higher performance.
For example, it is well known that translation of an indirect branch (e.g., function
return)
on a full binary translator incurs significant overhead due to a potential
table lookup on each execution. Our approach avoids this overhead.
The disadvantage of our approach is that we change the guest's address space
directly and have to thus monitor guest's accesses to our modified regions
(which we do using read/write tracing). As we demonstrate in this work, it is
possible to do this correctly and efficiently using our proposed optimizations.

Our in-place binary translation approach relies on the fixed-length word-aligned
nature of Power Architecture instructions. We ensure that a guest cannot possibly
jump to the middle of our translation by relying on this property. Because the
x86 architecture has variable-sized non-aligned instructions, in-place binary
translation is much harder (or perhaps impossible) to implement correctly
on x86.

We also rely on the ability to read/write trace guest pages by marking them
{\tt execute-only}. This is possible on embedded Power Architecture due to the
availability of separate
{\tt read-write-execute} page protection bits.
In contrast, the x86 architecture provides only {\tt read-only} and
{\tt no-execute} (NX) bits, which are less powerful, and insufficient
to implement {\tt execute-only} privileges at page granularity.

Subtle
differences in architectures greatly impact VMM design. We believe that our
approach is perhaps a misfit for the x86 architecture for reasons outlined above.
Similarly, a full binary translator is perhaps an overkill for the embedded
Power Architecture, given that our lightweight adaptive in-place binary translator
can achieve the same (or better) effect with less engineering effort.

\subsection{Other Related Work}
Binary translation has been previously used in various contexts, namely
cross-ISA portability\cite{bansal:osdi08, qemu:software}, hardware-based
performance acceleration\cite{transmeta_crusoe:chip}, runtime compiler
optimizations\cite{bala00dynamo}, program shepherding\cite{bruening04thesis},
testing and debugging\cite{valgrind}. Binary translation was
first used for efficient virtualization on the x86 architecture by
VMware\cite{adams:asplos06}, and our work
is perhaps closest to their approach. The difference is in the translator's design,
as previously discussed in Section~\ref{sec:comparison_with_full_bt}.

The recent extension to the Linux kernel for Power Architecture paravirtualization
contrasts with our approach. While the paravirtual modifications require extensive
changes to the Linux kernel, our approach can achieve comparable performance
with only host-side optimizations. Compared with the XXXX LOC included in the
paravirtual Linux extensions, our approach requires only XXX LOC to be added to
a Linux guest.

We present our experiments and results on a uniprocessor system but our
ideas are equally relevant to a multiprocessor system. For a multiprocessor
guest/host, these optimizations must be implemented for each virtual CPU (VCPU).
To reduce synchronization
overheads, separate translation and data caches need to be maintained for each VCPU.
This minimizes synchronization overheads at the potential cost of marginally higher
space overheads. We expect
our optimizations to show equivalent performance improvements on a multiprocessor.
\section{Conclusion}
\label{sec:conclusion}
We discuss the design and implementation of an efficient host-side virtualization
solution
for embedded Power Architecture processors. We propose and validate three important
optimizations
for efficient virtualization, namely {\em in-place binary translation and read/write
tracing}, {\em adaptive page resizing}, and {\em adaptive data mirroring}. 
The Linux/KVM-based prototype system developed on these ideas shows significant
performance improvements on common workloads, and compares favourably to
previously-proposed paravirtual approaches.
\bibliography{ppcvirt}
\bibliographystyle{abbrv}

\end{document}
